# 关于task2的一个总结
emmm我没有完成这个任务，我一直出现一个我看不懂的问题
![](https://pic.imgdb.cn/item/6718b7dad29ded1a8cb0be79.png)
就是我没有引用这个呀，为什么它会显示我引用了'OrderedDict'，我查了ai，他说让我仔细检查代码，但我自己检查了挺久的，感觉也没有jupytor上显示的错误
![](https://pic.imgdb.cn/item/6718ccded29ded1a8cc7b5df.png)
ai叫我写这段代码，我也是运行不了

## 简单思考
#### 计算机采用什么数据结构存储、处理图像？
像素阵列，用于表示图像。图像可以被看作是一个二维数组，其中每个元素代表一个像素
图像矩阵，在图像处理中，图像常常被表示为矩阵。矩阵中的每个元素对应于图像中的一个像素，并且包含该像素的强度或颜色信息。
#### 如何设计一个神经网络？一个神经网络通常包括哪些部分？
简单来说就是分为输入层，隐藏层和输出层
而隐藏层中包含着神经元
![基本架构](https://pic.imgdb.cn/item/67171118d29ded1a8ca21437.png)

##### 权重和偏置
权重：连接神经元的边，决定输入信号的重要性。每个连接都有一个权重值。
偏置：为每个神经元添加的常数项，允许激活函数在不同的点上移动。
简单来说就是这样
![权重](https://pic.imgdb.cn/item/67171bb9d29ded1a8cb4104c.png)
![](https://pic.imgdb.cn/item/67171264d29ded1a8ca3396b.jpg)
如图，像w就是权重决定着这个数据的重要程度，越重要他的权重占比就会越大，偏置就是图中的b

##### 激活函数
激活函数就不多说了，常见的函数可见上面
主要是讲一下在神经网络中如何的去选择激活函数
**先说结论一般在隐藏层中，都会选择relu函数
在输出层中根据实际情况来选择函数**
例如你要是要输出真假值的话就用逻辑回归，要是是一个线性回归的一个问题的话，就使用线性回归函数
拿线性回归函数来举例子，要是隐藏层使用线性回归方程，可以想象一下，到最后他只是一个线性回归的一个嵌套，我们根本没有必要设置隐藏层，我们可以直接使用一个比较复杂的线性回归就可以解决
要是使用逻辑回归来当隐藏层的激活函数的话，他就会有下面的结果![](https://pic.imgdb.cn/item/67163e17d29ded1a8c51d19c.png)
因为他前后都趋近于零，当我们使用梯度下降的时候，他就会有很多的局部最小点，那我们就很难得到一个比较小的一个成本函数


#### 损失函数损失函数
（Loss Function），也称为代价函数（Cost Function），是机器学习中用来评估模型预测值与真实值之间差异的函数。损失函数的目的是量化模型的预测误差
**简单来说，损失函数就是评价你的模型好坏的一个标准**
对于线性回归来说他的成本函数是
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
对于逻辑回归来说
$$
\text{Cross-Entropy Loss (Binary)} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$
对于softmax来说
$$
\text{Cross-Entropy Loss (Categorical)} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{m} y_{ic} \log(\hat{y}_{ic})
$$
![](https://pic.imgdb.cn/item/67171a05d29ded1a8cb0280c.png)

#### 优化器
优化器（Optimizer）是用于在训练过程中调整模型参数以最小化损失函数的算法
以下是常用的优化器（暂时我只学了这几种）

##### 随机梯度下降
与批量梯度下降每次更新参数时使用所有训练样本不同，SGD每次只随机选取一个样本或一对样本来更新模型的参数
$$
\theta = \theta - \alpha \nabla_\theta L(\theta; x^{(i)}, y^{(i)})
$$
每次参数更新只需要处理一个样本，因此计算和内存需求较低，运算会比较快，
但由于每次更新只基于一个样本，参数更新的步长可能会很大，导致收敛路径不稳定。
并且很依赖学习率，一般会与其他优化器一同使用

##### adam
简单来说，它可以根据实际情况改变学习率，例如要是成本函数一直沿着一个方向下降但速度较慢，那么他会增大学习率来加快收敛，要是学习率太大，离成本函数越来越远，那么他就会减少学习率，是函数收敛

#### 训练过程
**前向传播：输入数据通过各层进行计算，生成输出。
反向传播：通过计算损失函数的梯度，更新权重和偏置，以减少预测误差。**
#### 训练集 测试集 交叉验证测试集
训练集就是用来训练模型的一个数据集，机器会通过这个数据集来构造一个模型
交叉验证测试集就是我在已经得出一个模型中实验我是否会产生过拟合和欠拟合的问题，就是我再给出一个x，我通过这个模型来推测出y，来和我的真实值y比较，来算出损失函数
我会通过这个结果来改进我的模型
测试集就是最终用来测试最终的结果，他是不受干扰的
### 机器学习中的数据处理
#### 数据清洗：

**处理缺失值**：删除或填充缺失的数据。
**移除异常值**：识别并处理异常值，以减少它们对模型的影响。
#### 特征工程：
**特征选择**：从现有数据中选择最相关的特征。
**特征提取**：创建新的特征或转换现有特征以提供更多信息

#### 数据转换：

**归一化/标准化**：调整特征的尺度，使其对模型的影响一致。
**降维**：减少特征的数量，如使用主成分分析（PCA）

#### 数据划分：

**将数据集划分为训练集、验证集和测试集**


#### 什么是欠拟合？什么是过拟合？
### 过拟合和欠拟合和正则化
关于这几个概念我将引用几幅图来表示
![过拟合和欠拟合](https://pic.imgdb.cn/item/67177d5bd29ded1a8c4f0a61.jpg)
正如同他的字面意思意义欠拟合就是在训练集中都不能大概预测y值，也就是我们说的代价函数太大，不够拟合
过拟合就是在训练集上拟合的程度太高，反而可能在预测其他非训练集的时候会产生较大的误差，这就叫做过拟合
![正则化](https://pic.imgdb.cn/item/67177caad29ded1a8c4d90c5.jpg)
![正则化](https://pic.imgdb.cn/item/67177d39d29ded1a8c4ebf41.jpg)
正如同图中写出的任何损害优化的方法都是正则化，为什么要损害优化？因为过拟合了，就好像第二幅图一样，我要防止他的图像特别陡峭，我要使图像变得平滑一点，那么我要降低他的权重，这样我x在变化的时候他的变化就不会那么大，当然肯定不止这种方法
像图一的把训练集扩大一点，训练的更多，最后预测的会更加地准确，提前停止呢就是像图中所示，当我的验证集的错误率上升的时候，说明他就过拟合了，这个时候我们就可以停止了
## 解题思路
大概思路就是导入模块，检查设备，数据预处理（将图像转换为张量），选择测试集和训练集，然后对数据进行随机批次处理和打乱
然后定义神经网络模型
选择损失函数和测试函数，正向和反向传播

那个进阶题目我也做了，但因为我不能运行
我也不确定我写的对不对，就是画图的那个